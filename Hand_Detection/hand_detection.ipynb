{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from cvzone.HandTrackingModule import HandDetector\n",
    "from cvzone.ClassificationModule import Classifier\n",
    "import numpy as np\n",
    "import math\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터 수집"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "detector = HandDetector(maxHands=1)\n",
    "offset = 20\n",
    "imgSize =300\n",
    "\n",
    "folder = \"Data/Y\"\n",
    "counter = 0\n",
    "\n",
    "while True:\n",
    "    try:\n",
    "        ret, img = cap.read()\n",
    "        hands, img = detector.findHands(img)\n",
    "        if hands:\n",
    "            hand = hands[0]\n",
    "            x, y, w, h = hand[\"bbox\"]\n",
    "            \n",
    "            imgWhite = np.ones((imgSize, imgSize, 3), np.uint8)*255 # 저장되는 창\n",
    "            imgCrop = img[y-offset:y+h+offset, x-offset:x+w+offset] # 유동적인 창\n",
    "            \n",
    "            # 메인 창\n",
    "            aspectRatio = h/w\n",
    "            if aspectRatio>1:\n",
    "                k = imgSize/h\n",
    "                wCal = math.ceil(k*w)\n",
    "                imgResize = cv2.resize(imgCrop, (wCal, imgSize))\n",
    "                imgResizeShape = imgResize.shape\n",
    "                wGap = math.ceil((imgSize-wCal)/2)\n",
    "                imgWhite[:, wGap:wCal+wGap] = imgResize\n",
    "            else:\n",
    "                k = imgSize/w\n",
    "                hCal = math.ceil(k*h)\n",
    "                imgResize = cv2.resize(imgCrop, (imgSize, hCal))\n",
    "                imgResizeShape = imgResize.shape\n",
    "                hGap = math.ceil((imgSize-hCal)/2)\n",
    "                imgWhite[hGap:hCal+hGap, :] = imgResize\n",
    "            \n",
    "            cv2.imshow(\"ImageCrop\", imgCrop)\n",
    "            cv2.imshow(\"ImgWhite\", imgWhite)\n",
    "            \n",
    "        cv2.imshow(\"Image\", img)\n",
    "        \n",
    "        k = cv2.waitKey(1)\n",
    "        if k==ord(\"s\"): # s를 누르면 이미지 저장\n",
    "            counter += 1\n",
    "            cv2.imwrite(f\"./{folder}/Image_{time.time()}.jpg\", imgWhite)\n",
    "            print(counter)\n",
    "        if k==ord(\"q\"): # q를 누르면 프로그램 종료\n",
    "            break\n",
    "    except: # 경계선 밖으로 나가면 충돌 남\n",
    "        break\n",
    "    \n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Real Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    cap = cv2.VideoCapture(0)\n",
    "except:\n",
    "    cap = cv2.VideoCapture(1) \n",
    "cap.set(cv2.CAP_PROP_FRAME_WIDTH, 1280)\n",
    "cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 960)\n",
    "# hand detection    \n",
    "detector = HandDetector(maxHands=1)\n",
    "classifier = Classifier(\"./model/keras_model.h5\", \"./model/labels.txt\")\n",
    " \n",
    "offset = 20\n",
    "imgSize = 300\n",
    " \n",
    "labels = [chr(x).upper() for x in range(97, 123)]\n",
    "labels.remove(\"J\")\n",
    "labels.remove(\"Z\")\n",
    " \n",
    "while True:\n",
    "    try:\n",
    "        ret, img = cap.read()\n",
    "        imgOutput = img.copy()\n",
    "        hands, img = detector.findHands(img)\n",
    "        if hands:\n",
    "            x, y, w, h = hands[0]['bbox']\n",
    "            imgWhite = np.ones((imgSize, imgSize, 3), np.uint8)*255\n",
    "            imgCrop = img[y-offset:y+h+offset, x-offset:x+w+offset]\n",
    "            aspectRatio = h/w\n",
    "            if aspectRatio>1:\n",
    "                k = imgSize/h\n",
    "                wCal = math.ceil(k*w)\n",
    "                imgResize = cv2.resize(imgCrop, (wCal, imgSize))\n",
    "                wGap = math.ceil((imgSize-wCal)/2)\n",
    "                imgWhite[:, wGap:wCal+wGap] = imgResize\n",
    "                prediction, index = classifier.getPrediction(imgWhite, draw=False)\n",
    "                # print(prediction, index)\n",
    "            else:\n",
    "                k = imgSize/w\n",
    "                hCal = math.ceil(k*h)\n",
    "                imgResize = cv2.resize(imgCrop, (imgSize, hCal))\n",
    "                hGap = math.ceil((imgSize-hCal)/2)\n",
    "                imgWhite[hGap:hCal+hGap, :] = imgResize\n",
    "                prediction, index = classifier.getPrediction(imgWhite, draw=False)\n",
    "    \n",
    "            cv2.rectangle(imgOutput, (x-offset, y-offset-50), (x-offset+90, y-offset-50+50), (255, 0, 139), cv2.FILLED)\n",
    "            cv2.putText(imgOutput, labels[index], (x, y-26), cv2.FONT_HERSHEY_COMPLEX, 2, (255, 255, 255), 2)\n",
    "            cv2.rectangle(imgOutput, (x-offset, y-offset), (x+w+offset, y+h+offset), (255, 0, 139), 4)\n",
    "            # cv2.imshow(\"ImageCrop\", imgCrop)\n",
    "            # cv2.imshow(\"ImageWhite\", imgWhite)\n",
    "        if cv2.waitKey(1)==ord(\"q\"): break\n",
    "    except:\n",
    "        print(\"카메라가 경계선 밖으로 나갔습니다.\")\n",
    "        break\n",
    "    cv2.imshow(\"Sign Detectoin\", imgOutput)\n",
    "    \n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Streamlit\n",
    "import cv2\n",
    "import streamlit as st\n",
    "from cvzone.HandTrackingModule import HandDetector\n",
    "from cvzone.ClassificationModule import Classifier\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "detector = HandDetector(maxHands=1)\n",
    "classifier = Classifier(\"./model/keras_model.h5\", \"./model/labels.txt\")\n",
    " \n",
    "offset = 20\n",
    "imgSize = 300\n",
    " \n",
    "labels = [chr(x).upper() for x in range(97, 123)]\n",
    "labels.remove(\"J\")\n",
    "labels.remove(\"Z\")\n",
    "\n",
    "st.title(\"Real Time Classification\")\n",
    "run = st.checkbox('Run')\n",
    "FRAME_WINDOW = st.image([])\n",
    "\n",
    "try:\n",
    "    cap = cv2.VideoCapture(0)\n",
    "except:\n",
    "    cap = cv2.VideoCapture(1) \n",
    "\n",
    "while run:\n",
    "    _, frame = cap.read()\n",
    "    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    imgOutput = frame.copy()\n",
    "    hands, frame = detector.findHands(frame)\n",
    "    if hands:\n",
    "        x, y, w, h = hands[0]['bbox']\n",
    "        imgWhite = np.ones((imgSize, imgSize, 3), np.uint8)*255\n",
    "        imgCrop = frame[y-offset:y+h+offset, x-offset:x+w+offset]\n",
    "        aspectRatio = h/w\n",
    "        if aspectRatio>1:\n",
    "            k = imgSize/h\n",
    "            wCal = math.ceil(k*w)\n",
    "            imgResize = cv2.resize(imgCrop, (wCal, imgSize))\n",
    "            wGap = math.ceil((imgSize-wCal)/2)\n",
    "            imgWhite[:, wGap:wCal+wGap] = imgResize\n",
    "            prediction, index = classifier.getPrediction(imgWhite, draw=False)\n",
    "        else:\n",
    "            k = imgSize/w\n",
    "            hCal = math.ceil(k*h)\n",
    "            imgResize = cv2.resize(imgCrop, (imgSize, hCal))\n",
    "            hGap = math.ceil((imgSize-hCal)/2)\n",
    "            imgWhite[hGap:hCal+hGap, :] = imgResize\n",
    "            prediction, index = classifier.getPrediction(imgWhite, draw=False)\n",
    "        cv2.rectangle(imgOutput, (x-offset, y-offset-50), (x-offset+90, y-offset-50+50), (255, 0, 139), cv2.FILLED)\n",
    "        cv2.putText(imgOutput, labels[index], (x, y-26), cv2.FONT_HERSHEY_COMPLEX, 2, (255, 255, 255), 2)\n",
    "        cv2.rectangle(imgOutput, (x-offset, y-offset), (x+w+offset, y+h+offset), (255, 0, 139), 4)\n",
    "    FRAME_WINDOW.image(imgOutput)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('tf')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0dda229299321da99d0dbf63d1fbc8b04ca069326b7e356b1394d150d10bd532"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
